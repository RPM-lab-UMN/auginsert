<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AugInsert: Learning Robust Visual-Force Policies via Data Augmentation for Object Assembly Tasks">
  <meta name="keywords" content="Contact-Rich Manipulation, Imitation Learning, Data Augmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AugInsert</title>

  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script> -->

  <script>
    function updateRealWorldVideo() {
      var variation = document.getElementById("task-variation").value;

      // console.log("single", demo, task, inst)

      var video = document.getElementById("real-world-canonical-video");
      video.src = "media/results/real_world/" + 
                  "real_world_" +
                  variation +
                  ".mp4"
      video.playbackRate = 1.0;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://ryangdiaz.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://sites.google.com/view/geometric-peg-in-hole">
            Geometric Peg-in-Hole
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AugInsert: Learning Robust Visual-Force Policies via Data Augmentation for Object Assembly Tasks </h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://2025.ieee-icra.org">In Review at ICRA 2025</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://ryangdiaz.github.io">Ryan Diaz</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Adam Imdieke</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank">Vivek Veeriah</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://karthikdesingh.com">Karthik Desingh</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Minnesota - Twin Cities,</span>
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="paper/auginsert_icra2025.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

            <!-- Arxiv Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2209.05451"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span> -->

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="media/video/auginsert_icra2025_video_v2_compressed.mp4"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Talk Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://youtu.be/QcuXwmQgurE"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                </span>
                <span>Talk</span>
              </a>
            </span> -->

            <!-- Code Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://github.com/peract/peract"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span> -->

            </div>
          </div>
          <br>
          <br>


        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="100%">
            <source src="media/intro/sim_rolling_v2.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dperact">AugInsert</span> is coming soon!
        </h2>
      </div>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop height="100%">
            <source src="media/intro/1_handsan.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop height="100%">
            <source src="media/intro/2_food_bin.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/4_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay muted loop height="100%">
            <source src="media/intro/3_marker.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/5_stick.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/7_sweeping.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay muted loop height="100%">
            <source src="media/intro/6_blocks.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  We also train <b>one multi-task Transformer from scratch</b> on 7 real-world tasks with just <b>53 demos</b> in total.
</h2> -->


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper primarily focuses on learning robust visual-force policies in the context of high-precision object assembly
            tasks. Specifically, we focus on the <i>contact phase</i> of the assembly task where both objects (peg and hole) have made 
            contact and the objective lies in maneuvering the objects to complete the assembly. Moreover, we aim to learn contact-rich
            manipulation policies with multisensory inputs on limited expert data by expanding human demonstrations via online data
            augmentation.
          </p>
          <p>
            We develop a simulation environment with a dual-arm robot manipulator to evaluate the effect of augmented expert demonstration
            data. Our focus is on evaluating the robustness of our model with respect to certain task variations: <strong>grasp pose, peg/hole 
            shape object body shape, scene appearance, camera pose</strong>, and <strong>force-torque/proprioception noise</strong>. We show that 
            our proposed data augmentation method helps in learning a multisensory manipulation policy that is robust to unseen instances of these 
            variations, particularly physical variations such as <strong>grasp pose</strong>. Additionally, our ablative studies show the 
            significant contribution of force-torque data to the robustness of our model.
          </p>
          <p>
            For additional qualitative results and ablation studies, refer to the supplementary material on this webpage!
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>


  </div>


  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="media/video/auginsert_icra2025_video_v2_compressed.mp4"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3">Model Architecture</h2>
      <img src="media/figures/model_architecture.pdf" class="interpolation-image" 
      alt="Interpolate start reference image." />
      </br>
      </br>
        <p>
          Our behavior cloning framework implementation is based off of <a target="_blank" href="https://robomimic.github.io">Robomimic</a>. To encode our observations, we draw upon the success of <a target="_blank" href="https://www.mmintlab.com/research/vtt/">visuotactile transformer</a> encoders and utilize a similar attention-based mechanism for RGB and tactile modality fusion. Rather than performing self-attention directly with the input tokens, we found that introducing a cross-attention step similar to the <a target="_blank" href="https://deepmind.google/discover/blog/building-architectures-that-can-handle-the-worlds-data/">PerceiverIO</a> architecture seemed to work best for our task. We tokenize our inputs by computing linear projections of both visual patches (as in <a target="_blank" href="https://arxiv.org/abs/2010.11929">vision transformers</a>) for RGB inputs and individual readings per timestep for the force-torque input, and then add modality-specific position encodings. We then cross-attend these input tokens with a set of 8 learned latent vectors that then travel through a series of self-attention layers before ultimately being compressed and projected (as in VTT) to an output latent embedding. We encode proprioception with a multilayer perceptron to get an output embedding and concatenate both output embeddings to get the input to the policy network. The policy network is then a multilayer perceptron that outputs 3-dimensional delta actions.
        </p>
        <br/>
        <br/>

        <h2 class="title is-3">Experiments and Results</h2>

        <h3 class="title is-4">Simulation Experiments</h3>

        <p>
          Coming soon!
        </p>

        <!-- Offline vs. online augmentation -->

        <!-- Training set experiments visualization -->

        <!-- Attention visualization -->

        <!-- Canonical success rates for all experiments -->

        <br/>
        <br/>

        <h3 class="title is-4">Real World Experiments</h3>

        <!-- Canonical experiments graph -->

        <!-- Canonical experiments visualization -->

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Real World Rollouts: Model Trained on <i>No Variations</i></h3>

            Model evaluated on
            <div class="select is-small">
              <select id="task-variation" onchange="updateRealWorldVideo()">
              <option value="canonical" selected="selected">Canonical (No Variations)</option>
              <option value="grasp">Grasp Pose</option>
              <option value="body_shape">Object Body Shape</option>
              </select>
            </div>
            task variation.
            <br/>
            <br/>
            <br/>
            <br/>
            <video id="real-world-canonical-video"
                   muted
                   autoplay
                   loop
                   width="60%">
              <source src="media/results/real_world/real_world_canonical.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <!-- Modality input ablation graph -->

        <!-- </br>
        </br> -->

        <!-- <h3 class="title is-4">Action Predictions</h3>
        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Q-Prediction Examples</h3>

            Visualize predictions for   
            <div class="select is-small is-rounded">     
              <select id="single-menu-qpred" onchange="updateQpredVideo()">
              <option value="tomato" selected="selected">"put the tomatoes in the top bin"</option>
              <option value="stick">"hit the green ball with the stick"</option>
              <option value="handsan">"press the hand san"</option>
              <option value="tape">"put the tape in the top drawer"</option>
              </select>
            </div>
          </div> -->

      </div>
    </div>
  </div>
</section>

<!-- <video id="q-pred-video"
       muted
       autoplay
       loop
       width="100%">
  <source src="media/results/qpred/tomato.mp4"
          type="video/mp4">
</video> -->

<!-- <br>
<br> -->
<!-- <section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Emergent Properties</h2>

      <h3 class="title is-4">Tracking Objects</h3>
      A selected example of tracking an unseen hand sanitizer instance with an agent that was trained on a single object with 5 "press the handsan" demos. Since <span class="dperact">PerAct</span> focuses on actions, it doesn't need a complete representation of the bottle, and only has to predict <b><i>where</i> to press</b> the sanitizer.

      <video id="tracking-objects"
             muted
             autoplay
             loop
             width="99%">
        <source src="media/results/animations/handsan_tracking_v2.mp4" 
                type="video/mp4">
      </video>

    </div>

  </div>

</section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shridhar2022peract,
  title     = {Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation}, 
  author    = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year      = {2022},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            This website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by <a href="https://keunhong.com/">Keunhong Park</a> and <a href="https://peract.github.io">PerAct</a> made by <a href="https://mohitshridhar.com">Mohit Shridhar</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
